# üìò An√°lise do Gerenciamento de Branches do DeepResearch
## Equipe 1 - DeepResearch
| Nome                              | Matr√≠cula     | Descri√ß√£o da atividade                                                                                   |
|----------------------------------|---------------|-----------------------------------------------------------------------------------------------------------|
| √Ålex Santos Alencar              | 202300061518  | Realiza√ß√£o da an√°lise manual do projeto no GitHub.                                                       |
| Ellen Karolliny dos Santos       | 202300114326  | An√°lise dos resultados do microsoft/Phi-3-mini                             |
| Gabriel Luiz Santos Gama Barreto | 202300114335  | Aux√≠lio na constru√ß√£o do prompt dos modelos e an√°lise dos relat√≥rios gerados pelo deepseek coder. |
| Gabriel Ramos de Carvalho        | 202300061920  | Ajuda na cria√ß√£o do prompt. Apresenta√ß√£o e discuss√£o dos resultados obtidos a partir do MistralAI. |
| Jo√£o Andryel Santos Menezes      | 202300061652  | Escolha dos modelos, cria√ß√£o do prompt. An√°lise dos resultados e apresenta√ß√£o do Phi 3 mini.              |
| Larissa Batista dos Santos       | 202300061705  | Aux√≠lio na constru√ß√£o do tutorial, an√°lise e apresenta√ß√£o dos resultados obtidos utilizando o modelo Qwen2.5.                                        |
| Paloma dos Santos                | 202300061723  | Compara√ß√£o e an√°lise dos modelos selecionados. Ajuda na cria√ß√£o do modelo do documento .docx (Resposta da an√°lise e tutorial). |
| Rauany Ingrid Santos de Jesus    | 202300061760  | Introdu√ß√£o de Estrat√©gia de releases e modelo de fluxo de trabalho, bem como defini√ß√£o dos conceitos , exemplifica√ß√£o e defini√ß√£o do objetivo da atividade. Alimenta√ß√£o dos slides. Contribui√ß√£o no documento de an√°lise. |

### DeepResearch - https://github.com/Alibaba-NLP/DeepResearch (Reposit√≥rio do modelo analisado)
---
## V√≠deo de apresenta√ß√£o dos resultados e s√≠ntese do projeto
### [Acessar v√≠deo](https://drive.google.com/file/d/1mzDET2fs4TX35zRRkTenTXPzucnlFj1w/view?usp=drive_link)<br>
---
## üìö Sobre o Tutorial
---
Este tutorial apresenta como objetivo demonstrar o processo, passo a passo, de como identificar as estrat√©gias de branch do modelo de linguagem DeepResearch a partir de seu reposit√≥rio no github, utilizando e simulando quatro grandes modelos de linguagem (LLMs), executados a partir do Google Colab.
---
### A escolha dos quatro modelos: DeepSeek-Coder, Mistral, Phi-3 e Qwen, baseia-se em uma estrat√©gia de multiplas perspectivas, na qual cada arquitetura contribui com uma compet√™ncia espec√≠fica para a an√°lise do DeepResearch. O DeepSeek funciona como o especialista t√©cnico, focado em entender a l√≥gica por tr√°s de cada mudan√ßa no c√≥digo. J√° o Mistral 7B e o Qwen 2.5 7B oferecem um racioc√≠nio mais generalista e boa flexibilidade lingu√≠stica, o que os torna adequados para interpretar mensagens de commit, coment√°rios e documenta√ß√µes. Por fim, o Phi-3 Mini se destaca pela efici√™ncia, permitindo a an√°lise de grandes janelas de contexto (at√© 128 mil tokens) com menor custo computacional. Em conjunto, esses modelos possibilitam uma avalia√ß√£o da estrat√©gia de branches do DeepResearch a partir de m√∫ltiplas perspectivas, combinando suas particularidades em prol da an√°lise.
---
## üöÄ Op√ß√£o de Atalho (Recomendado)
Caso deseje **abrir diretamente o notebook no Google Colab**, sem seguir as etapas iniciais, utilize o link abaixo:

üîó **Acessar o Notebook no Colab:**  
https://colab.research.google.com/github/GabrielGamaUFS/Evolucao_Software_2025-2_DeepResearch_atividade2/blob/main/ESII_atv2.ipynb

‚û°Ô∏è **Se utilizar esta op√ß√£o, voc√™ pode seguir diretamente para o passo 2 e logo em seguida para o passo 4 do tutorial.**
---
## üß≠ Tutorial Completo

### 1. Abertura do Ambiente Google Colab (IDE)
Nesta etapa, deve-se acessar o ambiente Google Colab, dispon√≠vel no endere√ßo https://colab.google/, e criar um ‚ÄúNovo Notebook‚Äù ou ‚ÄúNew Notebook‚Äù.

<p align="center"> <img src="assets/tutorial1.jpg" width="70%"> </p>

---

### 2. Prepara√ß√£o do Ambiente

Nesse momento √© importante definir o uso da GPU no Colab. Para isso, acesse o menu ‚ÄúAmbiente de Execu√ß√£o‚Äù na parte superior da p√°gina, em seguida pressione ‚ÄúAlterar o tipo de ambiente de execu√ß√£o‚Äù, selecione a op√ß√£o ‚ÄúGPUs: T4‚Äù e clique em ‚Äúsalvar‚Äù, como segue as figuras:

<p align="center"> <img src="assets/tutorial2.jpg" width="70%"> </p>

<p align="center"> <img src="assets/tutorial3.jpg" width="70%"> </p>

---

### 3. Inser√ß√£o do c√≥digo-fonte
No ambiente do Google Colab, selecione a c√©lula do c√≥digo j√° existente. Caso n√£o haja uma c√©lula j√° criada, pressione a op√ß√£o ‚Äú+  C√≥digo‚Äù para inserir uma nova. 

<p align="center"> <img src="assets/tutorial4.jpg" width="70%"> </p>

Em seguida, copie cada bloco de c√≥digo abaixo e cole em cada cel√∫la no Google Colab:

<details>
  <summary><strong>üìå Clique para expandir a 1¬∞ c√©lula de c√≥digo</strong></summary>

```python
# 1. Instalar as bibliotecas necess√°rias

!pip install transformers accelerate torch bitsandbytes
```
</details>
<details>
  <summary><strong>üìå Clique para expandir a 2¬∞ c√©lula de c√≥digo</strong></summary>

```python
import os

# Define o diret√≥rio de destino no Colab
repo_dir = "/content/DeepResearch"

# Verifica se a pasta j√° existe antes de clonar
if not os.path.exists(repo_dir):
    print(f"A clonar https://github.com/Alibaba-NLP/DeepResearch para {repo_dir}...")

    !git clone https://github.com/Alibaba-NLP/DeepResearch.git
    print("Reposit√≥rio clonado com sucesso.")
else:
    print(f"Reposit√≥rio j√° existe em {repo_dir}.")
```
</details>
<details>
  <summary><strong>üìå Clique para expandir a 3¬∞ c√©lula de c√≥digo</strong></summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
                                                 # Reinicie a sess√£o sempre que trocar o modelo
# Nome do modelo que voc√™ quer
model_id = "deepseek-ai/deepseek-coder-6.7b-instruct"  # <--- Alterar pelo modelo escolhido:
                                                 # deepseek-ai/deepseek-coder-6.7b-instruct
print(f"Carregando {model_id} em 4-bit...")      # codellama/CodeLlama-7b-Instruct-hf
                                                 # mistralai/Mistral-7B-Instruct-v0.3
# --- Configura√ß√£o de 4-bit  ---                 # microsoft/Phi-3-mini-128k-instruct
bnb_config = BitsAndBytesConfig(                 # Qwen/Qwen2.5-7B-Instruct
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Carregar o tokenizador
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Carregar o modelo aplicando a configura√ß√£o de 4-bit
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,  # <-- Aplicando a configura√ß√£o de 4-bit
    device_map="auto"                # "auto" coloca o modelo na GPU
)

print("----------------------------------------------------------")
print(f"Modelo {model_id} carregado com sucesso em 4-bit!")
print("----------------------------------------------------------")
```
</details>
<details>
  <summary><strong>üìå Clique para expandir a 4¬∞ c√©lula de c√≥digo</strong></summary>

```python
import subprocess
import glob

# --- 1. FUN√á√ÉO PARA EXTRAIR INFORMA√á√ïES DO REPOSIT√ìRIO ---
def get_repo_context(repo_path):
    context_data = ""

    # A. Listar Branches e Tags (Indica versionamento)
    try:
        branches = subprocess.check_output(["git", "-C", repo_path, "branch", "-r"], text=True)
        tags = subprocess.check_output(["git", "-C", repo_path, "tag"], text=True)
        context_data += f"--- BRANCHES REMOTAS ---\n{branches}\n"
        context_data += f"--- TAGS (VERS√ïES) ---\n{tags}\n"
    except Exception as e:
        context_data += f"Erro ao ler git info: {e}\n"

    # B. Ler os √∫ltimos commits (Indica padr√£o de commit e merge)
    try:
        # Pega os √∫ltimos 20 commits formatados para mostrar merges
        logs = subprocess.check_output(
            ["git", "-C", repo_path, "log", "--graph", "--oneline", "-n", "20"],
            text=True
        )
        context_data += f"--- HIST√ìRICO DE COMMITS (GR√ÅFICO) ---\n{logs}\n"
    except:
        pass

    # C. Verificar Workflows do GitHub (Indica CI/CD e Release Automatizada)
    workflows = glob.glob(f"{repo_path}/.github/workflows/*.yml") + glob.glob(f"{repo_path}/.github/workflows/*.yaml")
    if workflows:
        context_data += "--- ARQUIVOS DE WORKFLOW (CI/CD) ENCONTRADOS ---\n"
        for wf in workflows:
            filename = os.path.basename(wf)
            context_data += f"Nome do arquivo: {filename}\n"
            # L√™ o conte√∫do dos workflows para entender o que eles fazem (ex: publish release)
            with open(wf, 'r') as f:
                context_data += f"Conte√∫do de {filename}:\n{f.read()}\n\n"
    else:
        context_data += "--- NENHUM WORKFLOW DE CI/CD ENCONTRADO ---\n"

    # D. Ler README e CONTRIBUTING (Busca regras escritas)
    for doc in ["README.md", "CONTRIBUTING.md", "RELEASE.md"]:
        path = os.path.join(repo_path, doc)
        if os.path.exists(path):
            with open(path, 'r') as f:
                content = f.read()
                # Trunca se for muito grande para n√£o estourar o contexto
                context_data += f"--- ARQUIVO {doc} ---\n{content[:2000]}...\n\n"

    return context_data

# --- 2. PREPARAR O PROMPT ---
print("Coletando dados do reposit√≥rio...")
repo_context = get_repo_context(repo_dir)

# Ajustamos o texto para ser mais diretivo e evitar que o modelo se alongue demais
instrucao_tarefa = """Voc√™ √© um Auditor de C√≥digo S√™nior e Especialista em DevOps.
Sua tarefa √© analisar os dados brutos de um reposit√≥rio Git fornecidos abaixo e extrair fatos reais.
Seja conciso e objetivo em cada ponto."""

corpo_dados = f"""Aqui est√£o os dados extra√≠dos do reposit√≥rio:
{repo_context}

Por favor, gere a an√°lise detalhada seguindo EXATAMENTE o formato abaixo."""

# Note que terminamos o prompt com o t√≠tulo do relat√≥rio para "puxar" a resposta do modelo
prompt = f"""### Instruction:
{instrucao_tarefa}

{corpo_dados}

### FORMATO DE RESPOSTA ESPERADO:
## Relat√≥rio: DeepResearch

**1. Modelo de Fluxo de Trabalho:**
* Veredito:
* Justificativa:

**2. Estrat√©gia de Releases:**
* Veredito:
* Justificativa:

**3. Resumo Geral:**

### Response:
## Relat√≥rio: DeepResearch"""

# --- EXECUTAR A INFER√äNCIA ---
print("Gerando an√°lise...")

input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(
    **input_ids,
    max_new_tokens=1024,   # Limite da resposta
    temperature=0.2,       # Baixa para manter o foco t√©cnico
    repetition_penalty=1.1, # Evita que ele repita as instru√ß√µes do prompt
    do_sample=True,
    top_p=0.9,
    eos_token_id=tokenizer.eos_token_id
)

# Decodificando
response = tokenizer.decode(outputs[0][input_ids.input_ids.shape[-1]:], skip_special_tokens=True)

# Exibimos o t√≠tulo manualmente j√° que o usamos para induzir a resposta
print("\n" + "="*50)
print("## Relat√≥rio: DeepResearch" + response)
print("="*50)
```
</details>

---

### 4. Execu√ß√£o dos Modelos de Linguagem (LLMs)
Ap√≥s o c√≥digo estar inserido, pressione o bot√£o ‚ÄúExecutar c√©lula‚Äù (√≠cone de play) ou ‚ÄúExecutar tudo‚Äù para processar o modelo e aguarde a conclus√£o da an√°lise realizada pelo LLM. 

<p align="center"> <img src="assets/tutorial5.jpg" width="70%"> </p>

---

### 5. Repeti√ß√£o da simula√ß√£o
O procedimento pode ser repetido para cada um dos quatro modelos de linguagem utilizados na simula√ß√£o, bastando substituir, na linha indicada abaixo, pelo modelo preferido.

<p align="center"> <img src="assets/tutorial6.jpg" width="70%"> </p>

Ap√≥s a substitui√ß√£o do modelo, √© necess√°rio reiniciar a sess√£o do ambiente, Para isso, clique na seta para baixo (‚ÄúMais a√ß√µes‚Äù) e em seguida ‚ÄúReinicar sess√£o‚Äù. Logo depois, retorne ao passo anterior.
<p align="center"> <img src="assets/tutorial7.jpg" width="70%"> </p>

---

## üìÑ Documenta√ß√£o da an√°lise dos LLMs

Acesse a vers√£o em PDF contendo:

- Introdu√ß√£o ao tema
- Tutorial
- A an√°lise detalhada
- Conclus√µes estruturadas
- Compara√ß√µes entre modelos
- Tabelas <br>
- Refer√™ncias

### [Acessar documento](docs/ESII-an√°lise-Atividade2.pdf)<br>

