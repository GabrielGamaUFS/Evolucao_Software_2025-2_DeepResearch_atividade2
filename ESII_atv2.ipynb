{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielGamaUFS/Evolucao_Software_2025-2_DeepResearch_atividade2/blob/main/ESII_atv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHfO_HQP2zLF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1. Instalar as bibliotecas necessárias\n",
        "\n",
        "!pip install transformers accelerate torch bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak8X1FOa3Agb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define o diretório de destino no Colab\n",
        "repo_dir = \"/content/DeepResearch\"\n",
        "\n",
        "# Verifica se a pasta já existe antes de clonar\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(f\"A clonar https://github.com/Alibaba-NLP/DeepResearch para {repo_dir}...\")\n",
        "\n",
        "    !git clone https://github.com/Alibaba-NLP/DeepResearch.git\n",
        "    print(\"Repositório clonado com sucesso.\")\n",
        "else:\n",
        "    print(f\"Repositório já existe em {repo_dir}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w92k2O_S3LvJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "                                                 # Reinicie a sessão sempre que trocar o modelo\n",
        "# Nome do modelo que você quer\n",
        "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"  # <--- Alterar pelo modelo escolhido:\n",
        "                                                 # deepseek-ai/deepseek-coder-6.7b-instruct\n",
        "print(f\"Carregando {model_id} em 4-bit...\")      # codellama/CodeLlama-7b-Instruct-hf\n",
        "                                                 # mistralai/Mistral-7B-Instruct-v0.3\n",
        "# --- Configuração de 4-bit  ---                 # microsoft/Phi-3-mini-128k-instruct\n",
        "bnb_config = BitsAndBytesConfig(                 # Qwen/Qwen2.5-7B-Instruct\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Carregar o tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Carregar o modelo aplicando a configuração de 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,  # <-- Aplicando a configuração de 4-bit\n",
        "    device_map=\"auto\"                # \"auto\" coloca o modelo na GPU\n",
        ")\n",
        "\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(f\"Modelo {model_id} carregado com sucesso em 4-bit!\")\n",
        "print(\"----------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjT7j1ho432j"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import glob\n",
        "\n",
        "# --- 1. FUNÇÃO PARA EXTRAIR INFORMAÇÕES DO REPOSITÓRIO ---\n",
        "def get_repo_context(repo_path):\n",
        "    context_data = \"\"\n",
        "\n",
        "    # A. Listar Branches e Tags (Indica versionamento)\n",
        "    try:\n",
        "        branches = subprocess.check_output([\"git\", \"-C\", repo_path, \"branch\", \"-r\"], text=True)\n",
        "        tags = subprocess.check_output([\"git\", \"-C\", repo_path, \"tag\"], text=True)\n",
        "        context_data += f\"--- BRANCHES REMOTAS ---\\n{branches}\\n\"\n",
        "        context_data += f\"--- TAGS (VERSÕES) ---\\n{tags}\\n\"\n",
        "    except Exception as e:\n",
        "        context_data += f\"Erro ao ler git info: {e}\\n\"\n",
        "\n",
        "    # B. Ler os últimos commits (Indica padrão de commit e merge)\n",
        "    try:\n",
        "        # Pega os últimos 20 commits formatados para mostrar merges\n",
        "        logs = subprocess.check_output(\n",
        "            [\"git\", \"-C\", repo_path, \"log\", \"--graph\", \"--oneline\", \"-n\", \"20\"],\n",
        "            text=True\n",
        "        )\n",
        "        context_data += f\"--- HISTÓRICO DE COMMITS (GRÁFICO) ---\\n{logs}\\n\"\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # C. Verificar Workflows do GitHub (Indica CI/CD e Release Automatizada)\n",
        "    workflows = glob.glob(f\"{repo_path}/.github/workflows/*.yml\") + glob.glob(f\"{repo_path}/.github/workflows/*.yaml\")\n",
        "    if workflows:\n",
        "        context_data += \"--- ARQUIVOS DE WORKFLOW (CI/CD) ENCONTRADOS ---\\n\"\n",
        "        for wf in workflows:\n",
        "            filename = os.path.basename(wf)\n",
        "            context_data += f\"Nome do arquivo: {filename}\\n\"\n",
        "            # Lê o conteúdo dos workflows para entender o que eles fazem (ex: publish release)\n",
        "            with open(wf, 'r') as f:\n",
        "                context_data += f\"Conteúdo de {filename}:\\n{f.read()}\\n\\n\"\n",
        "    else:\n",
        "        context_data += \"--- NENHUM WORKFLOW DE CI/CD ENCONTRADO ---\\n\"\n",
        "\n",
        "    # D. Ler README e CONTRIBUTING (Busca regras escritas)\n",
        "    for doc in [\"README.md\", \"CONTRIBUTING.md\", \"RELEASE.md\"]:\n",
        "        path = os.path.join(repo_path, doc)\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'r') as f:\n",
        "                content = f.read()\n",
        "                # Trunca se for muito grande para não estourar o contexto\n",
        "                context_data += f\"--- ARQUIVO {doc} ---\\n{content[:2000]}...\\n\\n\"\n",
        "\n",
        "    return context_data\n",
        "\n",
        "# --- 2. PREPARAR O PROMPT ---\n",
        "print(\"Coletando dados do repositório...\")\n",
        "repo_context = get_repo_context(repo_dir)\n",
        "\n",
        "# Ajustamos o texto para ser mais diretivo e evitar que o modelo se alongue demais\n",
        "instrucao_tarefa = \"\"\"Você é um Auditor de Código Sênior e Especialista em DevOps.\n",
        "Sua tarefa é analisar os dados brutos de um repositório Git fornecidos abaixo e extrair fatos reais.\n",
        "Seja conciso e objetivo em cada ponto.\"\"\"\n",
        "\n",
        "corpo_dados = f\"\"\"Aqui estão os dados extraídos do repositório:\n",
        "{repo_context}\n",
        "\n",
        "Por favor, gere a análise detalhada seguindo EXATAMENTE o formato abaixo.\"\"\"\n",
        "\n",
        "# Note que terminamos o prompt com o título do relatório para \"puxar\" a resposta do modelo\n",
        "prompt = f\"\"\"### Instruction:\n",
        "{instrucao_tarefa}\n",
        "\n",
        "{corpo_dados}\n",
        "\n",
        "### FORMATO DE RESPOSTA ESPERADO:\n",
        "## Relatório: DeepResearch\n",
        "\n",
        "**1. Modelo de Fluxo de Trabalho:**\n",
        "* Veredito:\n",
        "* Justificativa:\n",
        "\n",
        "**2. Estratégia de Releases:**\n",
        "* Veredito:\n",
        "* Justificativa:\n",
        "\n",
        "**3. Resumo Geral:**\n",
        "\n",
        "### Response:\n",
        "## Relatório: DeepResearch\"\"\"\n",
        "\n",
        "# --- EXECUTAR A INFERÊNCIA ---\n",
        "print(\"Gerando análise...\")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **input_ids,\n",
        "    max_new_tokens=1024,   # Limite da resposta\n",
        "    temperature=0.2,       # Baixa para manter o foco técnico\n",
        "    repetition_penalty=1.1, # Evita que ele repita as instruções do prompt\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decodificando\n",
        "response = tokenizer.decode(outputs[0][input_ids.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "# Exibimos o título manualmente já que o usamos para induzir a resposta\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"## Relatório: DeepResearch\" + response)\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}